# ==============================================================================
# Configuration for Study 3: Generation-Diagnosis Integrated System
# Task: Multi-task learning for lesion segmentation and classification,
#       leveraging the generated virtual HBP as a feature-enhancement channel.
# Inherits from: base_config.yaml
# ==============================================================================

defaults:
  - base_config

experiment_description: "Study 3: Trustworthy Diagnostic System with vHBP"

# ------------------------------------------------------------------------------
# 1. Data Specifics for Study 3 (Multi-modal Fusion)
# ------------------------------------------------------------------------------
data:
  # We use the same Gd-DTPA cohort as in Study 2 for diagnosis
  cohort_name: "cohort-B" 
  
  # The input is a super-dimensional tensor
  input_modalities:
    - "T1w_precontrast"
    - "T1w_arterial"
    - "T1w_portal"
    - "T1w_equilibrium"
    - "phys_Ktrans"
    - "phys_ve"
    - "virtual_hbp" # The vHBP generated by the Study 2 model
  
  # The ground truth for this task are the segmentation masks and class labels
  target_segmentation_mask: "lesion_mask"
  target_class_label: "lesion_label" # e.g., HCC, FNH, HCA, ICC, Hemangioma

# ------------------------------------------------------------------------------
# 2. Model Configuration: Multi-task Swin Transformer
# ------------------------------------------------------------------------------
model:
  name: "MultimodalDiagnosticSystem"
  
  # The backbone continues to be Swin Transformer for consistency
  backbone:
    name: "SwinUNETR"
    in_channels: 7 # Now we have 7 input channels
    img_size: [96, 96, 96]
    feature_size: 48
    # We can again use the weights from Study 1 for initialization
    pretrained_weights_path: "./outputs/study_1_acceleration/checkpoints/best_model.pth"
    # We will fine-tune the entire network on the diagnostic task
    freeze_weights: False 

  # The model has two separate heads for the two tasks
  segmentation_head:
    name: "ConvHead"
    out_channels: 6 # Num_classes (5) + 1 (background)

  classification_head:
    name: "MLPHead"
    in_features: 49152 # Flattened feature vector size from the backbone's bottleneck
    hidden_features: 512
    out_features: 5 # Number of lesion classes

  # Settings for the trustworthy AI mechanism
  uncertainty:
    # Enable Monte Carlo Dropout during inference
    mc_dropout_enabled: True
    # Number of forward passes for MC Dropout
    mc_dropout_samples: 25

# ------------------------------------------------------------------------------
# 3. Training Strategy for Multi-task Learning
# ------------------------------------------------------------------------------
training:
  epochs: 250
  batch_size: 2

  optimizer:
    name: "AdamW"
    learning_rate: 0.0001
  
  lr_scheduler:
    name: "CosineAnnealingLR"
    T_max: 250

  # The loss is a combination of segmentation and classification losses
  loss:
    components:
      - name: "DiceCELoss" # A robust loss for segmentation
        weight: 1.0
      - name: "CrossEntropyLoss" # Standard loss for classification
        weight: 0.5 # Adjust the weight to balance the two tasks